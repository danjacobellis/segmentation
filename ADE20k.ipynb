{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de63fb14-57f9-4b65-bcd6-d267c7f232f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "from torch.utils.data import DataLoader\n",
    "from mit_semseg.models.resnet import resnet50\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule \n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a96cda-4680-4e9a-8ef2-3df590923728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(sample):\n",
    "    img = sample['image']\n",
    "    label = sample['annotation']\n",
    "    sample['width'] = img.width\n",
    "    sample['height'] = img.height\n",
    "\n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "\n",
    "    img = img.resize((256,256),resample=PIL.Image.LANCZOS)\n",
    "    label = label.resize((256,256),resample=PIL.Image.LANCZOS)\n",
    "    \n",
    "    sample['image'] = Image().encode_example(img)\n",
    "    sample['annotation'] = Image().encode_example(label)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8864631-fff9-44a6-9dbc-48dc807a33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scene_parse_150\",split='train[0:1024]')\n",
    "dataset = dataset.map(prep_dataset)\n",
    "dataset.set_format(\"torch\")\n",
    "encoder = ModelBuilder().build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='')\n",
    "decoder = ModelBuilder().build_decoder(\n",
    "    arch='ppm',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='',\n",
    "    use_softmax=False)\n",
    "crit = nn.NLLLoss(ignore_index=-1)\n",
    "sm = SegmentationModule(encoder,decoder,crit)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95af785-384c-4efc-b1c6-2aeb98aeb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(dataloader):\n",
    "    x = batch['image']\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    x = x.to(torch.float)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bafd9c8-21cf-4860-bacb-cb3d02f3a9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sm.encoder(x, return_feature_maps=False)\n",
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a18bd4-fda0-4f71-8eee-a4343e126b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fe4789-3b6d-4c22-a3c8-25d02caa2f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 150, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
